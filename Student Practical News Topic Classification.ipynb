{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea8e19e-8ea2-4f12-aac2-1d707a601220",
   "metadata": {
    "id": "1ea8e19e-8ea2-4f12-aac2-1d707a601220"
   },
   "source": [
    "# Task: News Topic Classification with AG News\n",
    "\n",
    "## Objective\n",
    "Classify **news articles** into 4 categories (*World, Sports, Business, Sci/Tech*) using different **text representation methods**.\n",
    "\n",
    "<small>[AG News Classification Dataset on Kaggle](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)</small>\n",
    "    \n",
    "---\n",
    "\n",
    "## Step 1: Data Preparation\n",
    "- Load the **AG News dataset** (train.csv & test.csv).  \n",
    "- Combine the **title + description** into one text field.  \n",
    "- Apply **basic preprocessing**:\n",
    "  - Lowercase  \n",
    "  - Remove symbols/punctuation  \n",
    "  - Try stopwords removal or stemming â†’ compare results  \n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Representations to Try\n",
    "You must implement **all 5 methods** below:\n",
    "\n",
    "1. **Bag of Words (BoW)**  \n",
    "   - Represent each text as a count of words.  \n",
    "\n",
    "\n",
    "2. **TF-IDF**  \n",
    "   - Apply TF-IDF weighting instead of raw counts.  \n",
    "\n",
    "\n",
    "3. **N-grams (Bi/Tri-grams)**  \n",
    "   - Use bigrams and trigrams to capture context.   \n",
    "\n",
    "    \n",
    "4. **Word2Vec (Pretrained)**  \n",
    "   - Use pretrained embeddings (e.g., GoogleNews vectors).  \n",
    "   - Convert each document into a vector (average word embeddings).  \n",
    "\n",
    "    \n",
    "5. **Doc2Vec**  \n",
    "   - Train your own Doc2Vec model on the dataset.  \n",
    "   - Represent each document with its vector.  \n",
    "   \n",
    "---\n",
    "\n",
    "## Step 3: Try Two Classifiers\n",
    "For **each text representation method**, train **two different models** and compare:\n",
    "\n",
    "- **Logistic Regression**\n",
    "- **Naive Bayes** (or any other model of your choice, e.g., SVM, Decision Tree)\n",
    "\n",
    "Hint:  \n",
    "- Logistic Regression usually performs well on sparse features (BoW, TF-IDF, N-grams).  \n",
    "- Naive Bayes is very fast and works surprisingly well for text classification.  \n",
    "- Compare their accuracy for each representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Results Table\n",
    "Fill in your results:\n",
    "\n",
    "| Representation | Logistic Regression Acc | Naive Bayes Acc | Notes |\n",
    "|----------------|--------------------------|-----------------|-------|\n",
    "| BoW            |                          |                 |       |\n",
    "| TF-IDF         |                          |                 |       |\n",
    "| N-grams        |                          |                 |       |\n",
    "| Word2Vec       |                          |                 |       |\n",
    "| Doc2Vec        |                          |                 |       |\n",
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "1. Which method gave the best accuracy? Why?  \n",
    "2. Did N-grams improve performance compared to BoW?  \n",
    "3. How do pretrained embeddings (Word2Vec) compare to TF-IDF?  \n",
    "4. Which method is more efficient in terms of speed and memory?  \n",
    "5. If you had to build a **real news classifier**, which method would you choose and why?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1161818-6c0d-4ebc-aff7-6a2e3b83ec9a",
   "metadata": {
    "id": "d1161818-6c0d-4ebc-aff7-6a2e3b83ec9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mosta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "from gensim.models.doc2vec import Doc2Vec , TaggedDocument\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import nltk , re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f3308",
   "metadata": {},
   "source": [
    "1-Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de8adc83-1a58-4fe7-ae90-e9495345f3ff",
   "metadata": {
    "id": "de8adc83-1a58-4fe7-ae90-e9495345f3ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4            3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         Description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd8050c1-f570-415c-b764-855818335834",
   "metadata": {
    "id": "cd8050c1-f570-415c-b764-855818335834"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Fears for T N pension after talks Unions repre...\n",
       "1       The Race is On: Second Private Team Sets Launc...\n",
       "2       Ky. Company Wins Grant to Study Peptides (AP) ...\n",
       "3       Prediction Unit Helps Forecast Wildfires (AP) ...\n",
       "4       Calif. Aims to Limit Farm-Related Smog (AP) AP...\n",
       "                              ...                        \n",
       "7595    Around the world Ukrainian presidential candid...\n",
       "7596    Void is filled with Clement With the supply of...\n",
       "7597    Martinez leaves bitter Like Roger Clemens did ...\n",
       "7598    5 of arthritis patients in Singapore take Bext...\n",
       "7599    EBay gets into rentals EBay plans to buy the a...\n",
       "Name: text, Length: 7600, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- Combine the **title + description** into one text field.  \n",
    "train_df['text'] = train_df[\"Title\"] + \" \" + train_df['Description']\n",
    "test_df['text'] = test_df['Title'] + ' ' + test_df['Description']\n",
    "test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6b64ff7-36a5-4223-afe1-ff96c6ecce07",
   "metadata": {
    "id": "f6b64ff7-36a5-4223-afe1-ff96c6ecce07"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "train_labels = train_df[\"Class Index\"]\n",
    "test_labels = test_df[\"Class Index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "794d8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb601dd2",
   "metadata": {},
   "source": [
    "2-Represntations to Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1ae4b90-4f5e-4c59-8133-30a88df58cb1",
   "metadata": {
    "id": "b1ae4b90-4f5e-4c59-8133-30a88df58cb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bag of Words': 0.8938157894736842}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A) Bag of Words\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(preprocess)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(preprocess)\n",
    "\n",
    "bow = CountVectorizer(max_features=5000)\n",
    "X_train = bow.fit_transform(train_df[\"clean_text\"])\n",
    "X_test = bow.transform(test_df[\"clean_text\"])\n",
    "clf.fit(X_train, train_labels)\n",
    "results[\"Bag of Words\"] = accuracy_score(test_labels, clf.predict(X_test))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49e75f3a-6aeb-43af-85ee-383ddef5702e",
   "metadata": {
    "id": "49e75f3a-6aeb-43af-85ee-383ddef5702e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bag of Words': 0.8938157894736842, 'TF-IDF': 0.9031578947368422}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B) TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf.fit_transform(train_df[\"clean_text\"])\n",
    "X_test = tfidf.transform(test_df[\"clean_text\"])\n",
    "clf.fit(X_train, train_labels)\n",
    "results[\"TF-IDF\"] = accuracy_score(test_labels, clf.predict(X_test))\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad0cfd2e-f61a-4b99-89d7-e94fbc9d776b",
   "metadata": {
    "id": "ad0cfd2e-f61a-4b99-89d7-e94fbc9d776b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bag of Words': 0.8938157894736842,\n",
       " 'TF-IDF': 0.9031578947368422,\n",
       " 'N-grams': 0.7980263157894737}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C) N-grams\n",
    "ngram = TfidfVectorizer(ngram_range=(2,3), max_features=5000)\n",
    "X_train = ngram.fit_transform(train_df[\"clean_text\"])\n",
    "X_test = ngram.transform(test_df[\"clean_text\"])\n",
    "clf.fit(X_train, train_labels)\n",
    "results[\"N-grams\"] = accuracy_score(test_labels, clf.predict(X_test))\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1919a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bag of Words': 0.8938157894736842,\n",
       " 'TF-IDF': 0.9031578947368422,\n",
       " 'N-grams': 0.7980263157894737,\n",
       " 'Word2Vec': 0.8844736842105263}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D) Word2Vec\n",
    "\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-100\") \n",
    "def doc_vector(doc):\n",
    "    words = [w for w in doc.split() if w in w2v_model]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    return np.mean(w2v_model[words], axis=0)\n",
    "\n",
    "X_train = np.vstack([doc_vector(doc) for doc in train_df[\"clean_text\"]])\n",
    "X_test = np.vstack([doc_vector(doc) for doc in test_df[\"clean_text\"]])\n",
    "clf.fit(X_train, train_labels)\n",
    "results[\"Word2Vec\"] = accuracy_score(test_labels, clf.predict(X_test))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03efb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E) Doc2Vec\n",
    "train_tagged = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(train_df[\"clean_text\"])]\n",
    "\n",
    "doc2vec_model = Doc2Vec(vector_size=300, window=5, min_count=2, workers=4, epochs=20)\n",
    "doc2vec_model.build_vocab(train_tagged)\n",
    "doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "X_train = np.vstack([doc2vec_model.infer_vector(doc.split()) for doc in train_df[\"clean_text\"]])\n",
    "X_test = np.vstack([doc2vec_model.infer_vector(doc.split()) for doc in test_df[\"clean_text\"]])\n",
    "clf.fit(X_train, train_labels)\n",
    "results[\"Doc2Vec\"] = accuracy_score(test_labels, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dae0b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bag of Words': 0.8938157894736842, 'TF-IDF': 0.9031578947368422, 'N-grams': 0.7980263157894737, 'Word2Vec': 0.8844736842105263, 'Doc2Vec': 0.6647368421052632}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445775a8",
   "metadata": {},
   "source": [
    "3-Try two Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "960f138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, model, model_name, rep_name, results):\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    results.append({\"Representation\": rep_name, \"Model\": model_name, \"Accuracy\": acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d48ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# --------------------------\n",
    "# 1) Bag of Words\n",
    "# --------------------------\n",
    "bow = CountVectorizer(max_features=5000)\n",
    "X_train = bow.fit_transform(train_df[\"clean_text\"])\n",
    "X_test = bow.transform(test_df[\"clean_text\"])\n",
    "\n",
    "evaluate_model(X_train, X_test, train_labels, test_labels, LogisticRegression(max_iter=1000), \"Logistic Regression\", \"Bag of Words\", results)\n",
    "evaluate_model(X_train, X_test, train_labels, test_labels, MultinomialNB(), \"Naive Bayes\", \"Bag of Words\", results)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2) TF-IDF\n",
    "# --------------------------\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf.fit_transform(train_df[\"clean_text\"])\n",
    "X_test = tfidf.transform(test_df[\"clean_text\"])\n",
    "\n",
    "evaluate_model(X_train, X_test, train_labels, test_labels, LogisticRegression(max_iter=1000), \"Logistic Regression\", \"TF-IDF\", results)\n",
    "evaluate_model(X_train, X_test, train_labels, test_labels, MultinomialNB(), \"Naive Bayes\", \"TF-IDF\", results)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) N-grams (Bi/Tri-grams)\n",
    "# --------------------------\n",
    "ngram = TfidfVectorizer(ngram_range=(2,3), max_features=5000)\n",
    "X_train = ngram.fit_transform(train_df[\"clean_text\"])\n",
    "X_test = ngram.transform(test_df[\"clean_text\"])\n",
    "\n",
    "evaluate_model(X_train, X_test, train_labels, test_labels, LogisticRegression(max_iter=1000), \"Logistic Regression\", \"N-grams\", results)\n",
    "evaluate_model(X_train, X_test, train_labels, test_labels, MultinomialNB(), \"Naive Bayes\", \"N-grams\", results)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Doc2Vec\n",
    "# --------------------------\n",
    "X_train = np.vstack([doc2vec_model.infer_vector(doc.split()) for doc in train_df[\"clean_text\"]])\n",
    "X_test = np.vstack([doc2vec_model.infer_vector(doc.split()) for doc in test_df[\"clean_text\"]])\n",
    "\n",
    "#Can't use Naive bayes because of the negative values\n",
    "evaluate_model(X_train, X_test, train_labels, test_labels, LogisticRegression(max_iter=1000), \"Logistic Regression\", \"Doc2Vec\", results)\n",
    "\n",
    "# --------------------------\n",
    "# 5) Word2Vec (Pretrained)\n",
    "# --------------------------\n",
    "X_train = np.vstack([doc_vector(doc) for doc in train_df[\"clean_text\"]])\n",
    "X_test = np.vstack([doc_vector(doc) for doc in test_df[\"clean_text\"]])\n",
    "\n",
    "#Can't use Naive bayes because of the negative values\n",
    "evaluate_model(X_train, X_test, train_labels, test_labels, LogisticRegression(max_iter=1000), \"Logistic Regression\", \"Word2Vec\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3317779",
   "metadata": {},
   "source": [
    "4-Result Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ef88d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation  Logistic Regression  Naive Bayes\n",
      "  Bag of Words             0.893816     0.888684\n",
      "       Doc2Vec             0.666974          NaN\n",
      "       N-grams             0.798026     0.764605\n",
      "        TF-IDF             0.903158     0.889474\n",
      "      Word2Vec             0.884474          NaN\n"
     ]
    }
   ],
   "source": [
    "# Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Pivot into the required table format\n",
    "table = results_df.pivot(index=\"Representation\", columns=\"Model\", values=\"Accuracy\").reset_index()\n",
    "\n",
    "# Add empty Notes column\n",
    "table[\"Notes\"] = \"\"\n",
    "\n",
    "# Reorder columns to match your format\n",
    "table = table[[\"Representation\", \"Logistic Regression\", \"Naive Bayes\"]]\n",
    "\n",
    "print(table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5a9bb",
   "metadata": {},
   "source": [
    "Reflection Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189833f",
   "metadata": {},
   "source": [
    "1. Which method gave the best accuracy? Why?  \n",
    "- TF-IDF gave the best accuarcy, because it highlights the important words and cancel the noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a03058",
   "metadata": {},
   "source": [
    "2. Did N-grams improve performance compared to BoW?  \n",
    "- No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325dd92",
   "metadata": {},
   "source": [
    "3. How do pretrained embeddings (Word2Vec) compare to TF-IDF?  \n",
    "- TF-IDF was better "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3634022",
   "metadata": {},
   "source": [
    "4. Which method is more efficient in terms of speed and memory?  \n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0efc93",
   "metadata": {},
   "source": [
    "5. If you had to build a **real news classifier**, which method would you choose and why?  \n",
    "- I would TF-IDF because its efficient and works on large data set and would use the logistic regression with it because it has high accuarcy and it overpreforms BOW and N-grams"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
